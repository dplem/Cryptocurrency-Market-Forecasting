{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\r\n",
    "import json\r\n",
    "import requests\r\n",
    "import itertools\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "from datetime import datetime, timedelta\r\n",
    "import praw\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_request(uri, max_retries = 5):\r\n",
    "    def fire_away(uri):\r\n",
    "        response = requests.get(uri)\r\n",
    "        assert response.status_code == 200\r\n",
    "        return json.loads(response.content)\r\n",
    "    current_tries = 1\r\n",
    "    while current_tries < max_retries:\r\n",
    "        try:\r\n",
    "            time.sleep(1)\r\n",
    "            response = fire_away(uri)\r\n",
    "            return response\r\n",
    "        except:\r\n",
    "            time.sleep(1)\r\n",
    "            current_tries += 1\r\n",
    "    return fire_away(uri)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pull_posts_for(subreddit, start_at, end_at):\r\n",
    "    \r\n",
    "    def map_posts(posts):\r\n",
    "        return list(map(lambda post: {\r\n",
    "            'id': post['id'],\r\n",
    "            'created_utc': post['created_utc'],\r\n",
    "            'prefix': 't4_'\r\n",
    "        }, posts))\r\n",
    "    \r\n",
    "    SIZE = 500\r\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\r\n",
    "    \r\n",
    "    post_collections = map_posts( \\\r\n",
    "        make_request( \\\r\n",
    "            URI_TEMPLATE.format( \\\r\n",
    "                subreddit, start_at, end_at, SIZE))['data'])\r\n",
    "    n = len(post_collections)\r\n",
    "    while n == SIZE:\r\n",
    "        last = post_collections[-1]\r\n",
    "        new_start_at = last['created_utc'] - (10)\r\n",
    "        \r\n",
    "        more_posts = map_posts( \\\r\n",
    "            make_request( \\\r\n",
    "                URI_TEMPLATE.format( \\\r\n",
    "                    subreddit, new_start_at, end_at, SIZE))['data'])\r\n",
    "        \r\n",
    "        n = len(more_posts)\r\n",
    "        post_collections.extend(more_posts)\r\n",
    "\r\n",
    "    return post_collections"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def give_me_intervals(start_at, number_of_days_per_interval = 3):\r\n",
    "    \r\n",
    "    end_at = math.ceil(datetime.utcnow().timestamp())\r\n",
    "        \r\n",
    "    ## 1 day = 86400,\r\n",
    "    period = (86400 * number_of_days_per_interval)\r\n",
    "    end = start_at + period\r\n",
    "    yield (int(start_at), int(end))\r\n",
    "    padding = 1\r\n",
    "    while end <= end_at:\r\n",
    "        start_at = end + padding\r\n",
    "        end = (start_at - padding) + period\r\n",
    "        yield int(start_at), int(end)\r\n",
    "## test out the solution,\r\n",
    "start_at = math.floor(\\\r\n",
    "     (datetime.utcnow() - timedelta(days=365)).timestamp())\r\n",
    "list(give_me_intervals(start_at, 7))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set Reddit API Credentials\r\n",
    "reddit = praw.Reddit(\r\n",
    "     client_id=\"3gRvZgbUzCz_Tg\",\r\n",
    "     client_secret=\"BuNkAnAhJFZhtDMw303NyxiIonIkpg\",\r\n",
    "     user_agent=\"image-scraper\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the name of the directory to be created. Replace with your directory location.\r\n",
    "csv_dir = '../Data/Reddit_Comments/'\r\n",
    "\r\n",
    "# Define directory for parquet file.\r\n",
    "parquet_dir = '../Data/Parquet/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub_list = ['Cryptocurrency', 'Altcoin', 'Bitcoin', 'Ethereum', 'BasicAttentionToken', 'Best_of_Crypto', 'BitcoinMarkets', \r\n",
    "            'Blockchain', 'CryptoMarkets', 'CryptoTechnology', 'CryptoTrade', 'Algorand', 'Tezos', 'cosmosnetwork',\r\n",
    "            'Polkadot', 'Cardano', 'Ankr']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for x in sub_list:\r\n",
    "    subreddit = x\r\n",
    "    start_at = math.floor(\\\r\n",
    "    (datetime.utcnow() - timedelta(days=365)).timestamp())\r\n",
    "    posts = []\r\n",
    "    for interval in give_me_intervals(start_at, 7):\r\n",
    "        pulled_posts = pull_posts_for(\r\n",
    "            subreddit, interval[0], interval[1])\r\n",
    "    \r\n",
    "        posts.extend(pulled_posts)\r\n",
    "        time.sleep(.500)\r\n",
    "\r\n",
    "    # Pull reddit posts and comments\r\n",
    "    TIMEOUT_AFTER_COMMENT_IN_SECS = .350\r\n",
    "    posts_from_reddit = []\r\n",
    "    comments_from_reddit = []\r\n",
    "    for submission_id in np.unique([ post['id'] for post in posts ]):\r\n",
    "        submission = reddit.submission(id=submission_id)\r\n",
    "        posts_from_reddit.append(submission)\r\n",
    "        submission.comments.replace_more(limit=None)\r\n",
    "        for comment in submission.comments.list():\r\n",
    "            comments_from_reddit.append(comment)\r\n",
    "        \r\n",
    "            if TIMEOUT_AFTER_COMMENT_IN_SECS > 0:\r\n",
    "                time.sleep(TIMEOUT_AFTER_COMMENT_IN_SECS)\r\n",
    "\r\n",
    "    # Create DataFrame\r\n",
    "    p = pd.DataFrame(posts_from_reddit)\r\n",
    "    c = pd.DataFrame(comments_from_reddit)\r\n",
    "    \r\n",
    "    # Create date string for csv file name\r\n",
    "    timestr = time.strftime(\"%Y%m%d\")\r\n",
    "    \r\n",
    "    # Save dataframe to csv file\r\n",
    "    p.to_csv(csv_dir + subreddit + 'posts_'  + timestr + '.csv', index = False)\r\n",
    "    c.to_csv(csv_dir + subreddit + 'comments_'  + timestr + '.csv', index = False)\r\n",
    "\r\n",
    "    # Save dataframe to parquet file\r\n",
    "    p.to_parquet(parquet_dir + subreddit + 'posts_' + timestr + '.parquet', engine='fastparquet')\r\n",
    "    c.to_parquet(parquet_dir + subreddit + 'comments_' + timestr + '.parquet', engine='fastparquet')\r\n",
    "\r\n",
    "    time.sleep(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "subreddit = 'Siacoin'\r\n",
    "start_at = math.floor(\\\r\n",
    "    (datetime.utcnow() - timedelta(days=365)).timestamp())\r\n",
    "posts = []\r\n",
    "for interval in give_me_intervals(start_at, 7):\r\n",
    "    pulled_posts = pull_posts_for(\r\n",
    "        subreddit, interval[0], interval[1])\r\n",
    "    \r\n",
    "    posts.extend(pulled_posts)\r\n",
    "    time.sleep(.500)\r\n",
    "## ~ 4306\r\n",
    "print(len(posts))\r\n",
    "## ~ 4306\r\n",
    "print(len(np.unique([ post['id'] for post in posts ])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## WARNING: REDDIT WILL THROTTLE YOU IF YOU ARE ANNOYING! BE KIND!\r\n",
    "TIMEOUT_AFTER_COMMENT_IN_SECS = .350\r\n",
    "posts_from_reddit = []\r\n",
    "comments_from_reddit = []\r\n",
    "for submission_id in np.unique([ post['id'] for post in posts ]):\r\n",
    "    submission = reddit.submission(id=submission_id)\r\n",
    "    posts_from_reddit.append(submission)\r\n",
    "    submission.comments.replace_more(limit=None)\r\n",
    "    for comment in submission.comments.list():\r\n",
    "        comments_from_reddit.append(comment)\r\n",
    "        \r\n",
    "        if TIMEOUT_AFTER_COMMENT_IN_SECS > 0:\r\n",
    "            time.sleep(TIMEOUT_AFTER_COMMENT_IN_SECS)\r\n",
    "## ~ 4306\r\n",
    "print(len(posts_from_reddit))\r\n",
    "## ~ 35216\r\n",
    "print(len(comments_from_reddit))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create dataframe\r\n",
    "p = pd.DataFrame(posts_from_reddit)\r\n",
    "c = pd.DataFrame(comments_from_reddit)\r\n",
    "    \r\n",
    "# Create date string for csv file name\r\n",
    "timestr = time.strftime(\"%Y%m%d\")\r\n",
    "    \r\n",
    "# Save dataframe to csv file\r\n",
    "p.to_csv(csv_dir + subreddit + 'posts_' + timestr + '.csv', index = False)\r\n",
    "c.to_csv(csv_dir + subreddit + 'comments_' + timestr + '.csv', index = False)\r\n",
    "\r\n",
    "# Save dataframe to parquet file\r\n",
    "#p.to_parquet(parquet_dir + subreddit + '_' + timestr + '.parquet', engine='fastparquet')\r\n",
    "#c.to_parquet(parquet_dir + subreddit + '_' + timestr + '.parquet', engine='fastparquet')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
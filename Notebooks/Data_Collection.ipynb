{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection from Reddit Cryptocurrency Subreddits\r\n",
    "\r\n",
    "https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\r\n",
    "\r\n",
    "https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install pmaw"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "import math\r\n",
    "import json\r\n",
    "import requests\r\n",
    "import itertools\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "from datetime import datetime, timedelta\r\n",
    "import praw\r\n",
    "import pandas as pd\r\n",
    "from pmaw import PushshiftAPI\r\n",
    "import datetime as dt\r\n",
    "from time import sleep\r\n",
    "from Historic_Crypto import HistoricalData, Cryptocurrencies"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# Define the name of the directory to be created. Replace with your directory location.\r\n",
    "csv_dir = '../Data/Reddit_Comments/'\r\n",
    "\r\n",
    "# Define directory for parquet file.\r\n",
    "parquet_dir = '../Data/Parquet/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "sub_list = ['Bitcoin']\r\n",
    "\r\n",
    "#            'Cryptocurrency, 'Altcoin', 'Bitcoin', 'Ethereum', 'BasicAttentionToken', 'Best_of_Crypto', 'BitcoinMarkets', \r\n",
    "#            'Blockchain', 'CryptoMarkets', 'CryptoTechnology', 'CryptoTrade', 'Algorand', 'Tezos', 'cosmosnetwork',\r\n",
    "#            'Polkadot', 'Cardano', 'Ankr']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "day = dt.timedelta(days=1)\r\n",
    "\r\n",
    "for x in sub_list:\r\n",
    "\r\n",
    "    #set date intervals\r\n",
    "    before = dt.datetime(2021,9,10,0,0).timestamp()\r\n",
    "    after = dt.datetime(2021,9,9,0,0).timestamp()\r\n",
    "    end = dt.datetime(2021,8,9,0,0).timestamp()\r\n",
    "\r\n",
    "    # Convert timestamp to datetime.datetime\r\n",
    "    before_dt = dt.datetime.fromtimestamp(before)\r\n",
    "    after_dt = dt.datetime.fromtimestamp(after)\r\n",
    "    end_dt = dt.datetime.fromtimestamp(end)\r\n",
    "\r\n",
    "    # Lowers date by 1 day for every iteration of the loop\r\n",
    "    while before_dt >= end_dt:\r\n",
    "        before_dt = int((before_dt - day).timestamp())\r\n",
    "        after_dt = int((after_dt - day).timestamp())\r\n",
    "        \r\n",
    "        subreddit = x\r\n",
    "\r\n",
    "        # pmaw python library to pull comments\r\n",
    "        api = PushshiftAPI()\r\n",
    "        subreddit = x\r\n",
    "        limit=100000\r\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before_dt, after=after_dt)\r\n",
    "        print(f'Retrieved {len(comments)} comments from Pushshift')\r\n",
    "\r\n",
    "        # Convert timestamp to datetime.datetime\r\n",
    "        before_dt = dt.datetime.fromtimestamp(before_dt)\r\n",
    "        after_dt = dt.datetime.fromtimestamp(after_dt)\r\n",
    "        end_dt = dt.datetime.fromtimestamp(end)\r\n",
    "\r\n",
    "        # Create dataframe\r\n",
    "        comments_df = pd.DataFrame(comments)\r\n",
    "\r\n",
    "        # Create list of columns to keep\r\n",
    "        cols = ['author', 'author_fullname', 'author_premium', 'body', 'collapsed_reason_code', 'comment_type', 'created_utc', 'score',\r\n",
    "                'id', 'parent_id', 'permalink']\r\n",
    "\r\n",
    "        # Make new dataframe with above columns\r\n",
    "        df = comments_df[cols].copy()\r\n",
    "\r\n",
    "        # Replace all characters except for letters and numbers\r\n",
    "        #comments_df['body'] = comments_df['body'].str.replace(\"(?i)[^0-9a-z!?.;,@' -]\",' ')\r\n",
    "\r\n",
    "        # Clean datetime for csv filename\r\n",
    "        date = str(after_dt).split(' 00:00:00', 1)[0]\r\n",
    "\r\n",
    "        # Export dataframe to csv\r\n",
    "        df.to_csv('../Data/Reddit_Comments/Bitcoin/' + subreddit + '_' + date + '.csv', header=True, index=False, columns=list(df.axes[1]))\r\n",
    "\r\n",
    "        # Export to pickle file\r\n",
    "        #df.to_pickle('../Data/Reddit_Comments/Cryptocurrency_pkl/' + subreddit + '_' + date + '.csv')\r\n",
    "\r\n",
    "        # sleep for n seconds\r\n",
    "        sleep(5)\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "93206 result(s) not found in Pushshift\n",
      "Total:: Success Rate: 100.00% - Requests: 50 - Batches: 5 - Items Remaining: 1795\n",
      "Retrieved 4999 comments from Pushshift\n",
      "89621 result(s) not found in Pushshift\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "day = dt.timedelta(days=1)\r\n",
    "\r\n",
    "for x in sub_list:\r\n",
    "\r\n",
    "    #set date intervals\r\n",
    "    before = int(dt.datetime(2021,9,10,0,0).timestamp())\r\n",
    "    after = int(dt.datetime(2021,9,9,0,0).timestamp())\r\n",
    "    end = int(dt.datetime(2021,8,9,0,0).timestamp())\r\n",
    "\r\n",
    "    # Convert timestamp to datetime.datetime\r\n",
    "    before_dt = dt.datetime.fromtimestamp(before)\r\n",
    "    after_dt = dt.datetime.fromtimestamp(after)\r\n",
    "    end_dt = dt.datetime.fromtimestamp(end)\r\n",
    "\r\n",
    "    while before_dt >= end_dt:\r\n",
    "\r\n",
    "        before_dt = int(before_dt.timestamp())\r\n",
    "        after_dt = int(after_dt.timestamp())\r\n",
    "        \r\n",
    "        subreddit = x\r\n",
    "\r\n",
    "        # pmaw python library to pull comments\r\n",
    "        api = PushshiftAPI()\r\n",
    "        subreddit = x\r\n",
    "        limit=100000\r\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before_dt, after=after_dt)\r\n",
    "        print(f'Retrieved {len(comments)} comments from Pushshift')\r\n",
    "\r\n",
    "        # Convert timestamp to datetime.datetime\r\n",
    "        before_dt = dt.datetime.fromtimestamp(before_dt)\r\n",
    "        after_dt = dt.datetime.fromtimestamp(after_dt)\r\n",
    "\r\n",
    "        # Lowers date by 1 day for every iteration of the loop\r\n",
    "        before_dt = (before_dt - day).timestamp()\r\n",
    "        after_dt = (after_dt - day).timestamp()\r\n",
    "\r\n",
    "        # Convert timestamp to datetime.datetime\r\n",
    "        before_dt = dt.datetime.fromtimestamp(before_dt)\r\n",
    "        after_dt = dt.datetime.fromtimestamp(after_dt)\r\n",
    "\r\n",
    "        # Create dataframe\r\n",
    "        comments_df = pd.DataFrame(comments)\r\n",
    "\r\n",
    "        # Create list of columns to keep\r\n",
    "        cols = ['author', 'author_fullname', 'author_premium', 'body', 'collapsed_reason_code', 'comment_type', 'created_utc', 'score',\r\n",
    "                'id', 'parent_id', 'permalink']\r\n",
    "\r\n",
    "        # Make new dataframe with above columns\r\n",
    "        df = comments_df[cols].copy()\r\n",
    "\r\n",
    "        # Replace all characters except for letters and numbers\r\n",
    "        #comments_df['body'] = comments_df['body'].str.replace(\"(?i)[^0-9a-z!?.;,@' -]\",' ')\r\n",
    "\r\n",
    "        # Clean datetime for csv filename\r\n",
    "        date = str(after_dt).split(' 00:00:00', 1)[0]\r\n",
    "\r\n",
    "        # Export dataframe to csv\r\n",
    "        df.to_csv('../Data/Reddit_Comments/Bitcoin/' + subreddit + '_' + date + '.csv', header=True, index=False, columns=list(df.axes[1]))\r\n",
    "\r\n",
    "        # Export to pickle file\r\n",
    "        #df.to_pickle('../Data/Reddit_Comments/Cryptocurrency_pkl/' + subreddit + '_' + date + '.csv')\r\n",
    "\r\n",
    "        # sleep for n seconds\r\n",
    "        sleep(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "93206 result(s) not found in Pushshift\n",
      "Total:: Success Rate: 100.00% - Requests: 81 - Batches: 9 - Items Remaining: 10\n",
      "10 result(s) not found in Pushshift\n",
      "Retrieved 6784 comments from Pushshift\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/Reddit_Comments/Bitcoin/Bitcoin_2021-09-07.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-0292b55638f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# Export dataframe to csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/Reddit_Comments/Bitcoin/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msubreddit\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Export to pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/Reddit_Comments/Bitcoin/Bitcoin_2021-09-07.csv'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cryptocurrency Historical Price Data Collection\r\n",
    "\r\n",
    "https://pypi.org/project/Historic-Crypto/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install Historic-Crypto"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting Historic-Crypto\n",
      "  Downloading Historic_Crypto-0.1.4-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user1\\anaconda3\\lib\\site-packages (from Historic-Crypto) (4.59.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user1\\anaconda3\\lib\\site-packages (from Historic-Crypto) (1.2.4)\n",
      "Requirement already satisfied: requests in c:\\users\\user1\\anaconda3\\lib\\site-packages (from Historic-Crypto) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from pandas->Historic-Crypto) (1.20.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from pandas->Historic-Crypto) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from pandas->Historic-Crypto) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->Historic-Crypto) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from requests->Historic-Crypto) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from requests->Historic-Crypto) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from requests->Historic-Crypto) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from requests->Historic-Crypto) (1.26.4)\n",
      "Installing collected packages: Historic-Crypto\n",
      "Successfully installed Historic-Crypto-0.1.4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Get full list of cryptocurrencies\r\n",
    "data = Cryptocurrencies(extended_output=False).find_crypto_pairs()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Connected to the CoinBase Pro API.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# List of cryptocurrency ticker symbols\r\n",
    "c_list = ['XTZ', 'ATOM', 'ALGO', 'ADA', 'ENJ', 'DOGE', 'DOT', 'SOL', 'BTC', 'ETH', 'LTC', \r\n",
    "          'BCH', 'UNI', 'LINK', 'AAVE', 'XLM', 'BAT']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "for x in c_list:\r\n",
    "    \r\n",
    "    # Export historical data\r\n",
    "    new = HistoricalData(x + '-USD',86400,'2014-01-01-00-00').retrieve_data()\r\n",
    "\r\n",
    "    # Save dataframe to csv\r\n",
    "    new.to_csv('../Data/Historical/' + x + '.csv', header=True)\r\n",
    "\r\n",
    "    sleep(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'XTZ-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XTZ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XTZ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XTZ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XTZ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XTZ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XTZ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'ATOM-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ATOM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ATOM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ATOM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ATOM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ATOM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ATOM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ATOM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'ALGO-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ALGO-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ALGO-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ALGO-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ALGO-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ALGO-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ALGO-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'ADA-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ADA-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ADA-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ADA-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ADA-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ADA-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ADA-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ADA-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ADA-USD' beginning at 2014-01-01-00-00. Trying a later date:'2019-10-02T00:00:00'\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'ENJ-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ENJ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ENJ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ENJ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ENJ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ENJ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ENJ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ENJ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ENJ-USD' beginning at 2014-01-01-00-00. Trying a later date:'2019-10-02T00:00:00'\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'DOGE-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2019-10-02T00:00:00'\n",
      "Data for chunk 8 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOGE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2020-07-28T00:00:00'\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'DOT-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2019-10-02T00:00:00'\n",
      "Data for chunk 8 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'DOT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2020-07-28T00:00:00'\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'SOL-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2019-10-02T00:00:00'\n",
      "Data for chunk 8 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'SOL-USD' beginning at 2014-01-01-00-00. Trying a later date:'2020-07-28T00:00:00'\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'BTC-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "Data for chunk 2 of 9 extracted\n",
      "Data for chunk 3 of 9 extracted\n",
      "Data for chunk 4 of 9 extracted\n",
      "Data for chunk 5 of 9 extracted\n",
      "Data for chunk 6 of 9 extracted\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'ETH-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ETH-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'ETH-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "Data for chunk 3 of 9 extracted\n",
      "Data for chunk 4 of 9 extracted\n",
      "Data for chunk 5 of 9 extracted\n",
      "Data for chunk 6 of 9 extracted\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'LTC-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LTC-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LTC-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LTC-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "Data for chunk 4 of 9 extracted\n",
      "Data for chunk 5 of 9 extracted\n",
      "Data for chunk 6 of 9 extracted\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'BCH-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BCH-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BCH-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BCH-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BCH-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "Data for chunk 5 of 9 extracted\n",
      "Data for chunk 6 of 9 extracted\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'UNI-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'UNI-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'UNI-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'UNI-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'UNI-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'UNI-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'UNI-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'UNI-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'UNI-USD' beginning at 2014-01-01-00-00. Trying a later date:'2019-10-02T00:00:00'\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'LINK-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LINK-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LINK-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LINK-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LINK-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LINK-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'LINK-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'AAVE-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'AAVE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'AAVE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'AAVE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'AAVE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'AAVE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'AAVE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'AAVE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'AAVE-USD' beginning at 2014-01-01-00-00. Trying a later date:'2019-10-02T00:00:00'\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'XLM-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XLM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XLM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XLM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XLM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XLM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'XLM-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "Data for chunk 7 of 9 extracted\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n",
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'BAT-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BAT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-01-01T00:00:00'\n",
      "Data for chunk 1 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BAT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2014-10-28T00:00:00'\n",
      "Data for chunk 2 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BAT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2015-08-24T00:00:00'\n",
      "Data for chunk 3 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BAT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2016-06-19T00:00:00'\n",
      "Data for chunk 4 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BAT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2017-04-15T00:00:00'\n",
      "Data for chunk 5 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BAT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-02-09T00:00:00'\n",
      "Data for chunk 6 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BAT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2018-12-06T00:00:00'\n",
      "Data for chunk 7 of 9 extracted\n",
      "CoinBase Pro API did not have any data available for 'BAT-USD' beginning at 2014-01-01-00-00. Trying a later date:'2019-10-02T00:00:00'\n",
      "Data for chunk 8 of 9 extracted\n",
      "Data for chunk 9 of 9 extracted\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Live Cryptocurrency data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "from Historic_Crypto import LiveCryptoData\r\n",
    "\r\n",
    "new =  LiveCryptoData('ATOM-USD').return_data()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'int'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipywidgets\\widgets\\widget.py\u001b[0m in \u001b[0;36m_ipython_display_\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[1;34m\"\"\"Called when `IPython.display.display` is called on the widget.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m         \u001b[0mplaintext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaintext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m110\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m             \u001b[0mplaintext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplaintext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'â€¦'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self, pretty)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pbar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTqdmHBox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_meter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_repr_json_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mformat_meter\u001b[1;34m(n, total, elapsed, ncols, prefix, ascii, unit, unit_scale, rate, bar_format, postfix, unit_divisor, initial, colour, **extra_kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;31m# Formatting progress bar space available for bar's display\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             full_bar = Bar(frac,\n\u001b[1;32m--> 533\u001b[1;33m                            \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnobar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mncols\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m                            \u001b[0mcharset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mASCII\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mascii\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mascii\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mBar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUTF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m                            colour=colour)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'int'"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'int'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\lib\\pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    389\u001b[0m                             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_repr_pretty_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                                 \u001b[1;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                                 \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'__repr__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m_repr_pretty_\u001b[1;34m(self, pp, *_, **__)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self, pretty)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pbar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTqdmHBox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_meter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_repr_json_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0m__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mformat_meter\u001b[1;34m(n, total, elapsed, ncols, prefix, ascii, unit, unit_scale, rate, bar_format, postfix, unit_divisor, initial, colour, **extra_kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;31m# Formatting progress bar space available for bar's display\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             full_bar = Bar(frac,\n\u001b[1;32m--> 533\u001b[1;33m                            \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnobar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mncols\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m                            \u001b[0mcharset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mASCII\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mascii\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mascii\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mBar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUTF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m                            colour=colour)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'int'"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/json": {
       "n": 0,
       "total": 4,
       "elapsed": 0.08022689819335938,
       "ncols": "1000px",
       "nrows": null,
       "prefix": "tqdm 400px",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write to Postgres database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install psycopg2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#from sqlalchemy import create_engine\r\n",
    "#import psycopg2 \r\n",
    "#import io"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#engine = create_engine('postgresql+psycopg2://username:password@host:port/database')\r\n",
    "\r\n",
    "#df.head(0).to_sql('table_name', engine, if_exists='replace',index=False) #drops old table and creates new empty table\r\n",
    "\r\n",
    "#conn = engine.raw_connection()\r\n",
    "#cur = conn.cursor()\r\n",
    "#output = io.StringIO()\r\n",
    "#df.to_csv(output, sep='\\t', header=False, index=False)\r\n",
    "#output.seek(0)\r\n",
    "#contents = output.getvalue()\r\n",
    "#cur.copy_from(output, 'table_name', null=\"\") # null values become ''\r\n",
    "#conn.commit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write pandas dataframe to parquet file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install pyarrow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write Pandas Dataframe to parquet\r\n",
    "#import pyarrow as pa\r\n",
    "#import pyarrow.parquet as pq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert DataFrame to Apache Arrow Table\r\n",
    "#table = pa.Table.from_pandas(comments_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parquet write table\r\n",
    "#pq.write_table(table, 'file_name.parquet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parquet with GZIP compression\r\n",
    "#pq.write_table(table, '../Data/Reddit_Comments/Cryptocurrency_09012021.parquet', compression='GZIP')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create dataframe\r\n",
    "#p = pd.DataFrame(posts_from_reddit)\r\n",
    "#c = pd.DataFrame(comments_from_reddit)\r\n",
    "    \r\n",
    "# Create date string for csv file name\r\n",
    "#timestr = time.strftime(\"%Y%m%d\")\r\n",
    "    \r\n",
    "# Save dataframe to csv file\r\n",
    "#p.to_csv(csv_dir + subreddit + 'posts_' + timestr + '.csv', index = False)\r\n",
    "#c.to_csv(csv_dir + subreddit + 'comments_' + timestr + '.csv', index = False)\r\n",
    "\r\n",
    "# Save dataframe to parquet file\r\n",
    "#p.to_parquet(parquet_dir + subreddit + '_' + timestr + '.parquet', engine='fastparquet')\r\n",
    "#c.to_parquet(parquet_dir + subreddit + '_' + timestr + '.parquet', engine='fastparquet')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "825893eda577408078809c25d9ed95f592e592429a99ce56af90a73b72386c66"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
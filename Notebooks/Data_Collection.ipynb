{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection from Reddit Cryptocurrency Subreddits\r\n",
    "\r\n",
    "https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\r\n",
    "\r\n",
    "https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!pip install pmaw"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import math\r\n",
    "import json\r\n",
    "import requests\r\n",
    "import itertools\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "from datetime import datetime, timedelta\r\n",
    "import praw\r\n",
    "import pandas as pd\r\n",
    "from pmaw import PushshiftAPI\r\n",
    "import datetime as dt\r\n",
    "from time import sleep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Define the name of the directory to be created. Replace with your directory location.\r\n",
    "csv_dir = '../Data/Reddit_Comments/'\r\n",
    "\r\n",
    "# Define directory for parquet file.\r\n",
    "parquet_dir = '../Data/Parquet/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sub_list = ['Blockchain']\r\n",
    "\r\n",
    "# 'Cryptocurrency, 'Altcoin', 'Bitcoin', 'Ethereum', 'BasicAttentionToken', 'Best_of_Crypto', 'BitcoinMarkets', \r\n",
    "#            'Blockchain', 'CryptoMarkets', 'CryptoTechnology', 'CryptoTrade', 'Algorand', 'Tezos', 'cosmosnetwork',\r\n",
    "#            'Polkadot', 'Cardano', 'Ankr']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "day = dt.timedelta(days=1)\r\n",
    "\r\n",
    "for x in sub_list:\r\n",
    "\r\n",
    "    #set date intervals\r\n",
    "    before = dt.datetime(2021,9,10,0,0).timestamp()\r\n",
    "    after = dt.datetime(2021,9,9,0,0).timestamp()\r\n",
    "    end = dt.datetime(2021,8,9,0,0).timestamp()\r\n",
    "\r\n",
    "    # Convert timestamp to datetime.datetime\r\n",
    "    before_dt = dt.datetime.fromtimestamp(before)\r\n",
    "    after_dt = dt.datetime.fromtimestamp(after)\r\n",
    "    end_dt = dt.datetime.fromtimestamp(end)\r\n",
    "\r\n",
    "    # Lowers date by 1 day for every iteration of the loop\r\n",
    "    while before_dt >= end_dt:\r\n",
    "        before_dt = int((before_dt - day).timestamp())\r\n",
    "        after_dt = int((after_dt - day).timestamp())\r\n",
    "        \r\n",
    "        subreddit = x\r\n",
    "\r\n",
    "        # pmaw python library to pull comments\r\n",
    "        api = PushshiftAPI()\r\n",
    "        subreddit = x\r\n",
    "        limit=100000\r\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before_dt, after=after_dt)\r\n",
    "        print(f'Retrieved {len(comments)} comments from Pushshift')\r\n",
    "\r\n",
    "        # Convert timestamp to datetime.datetime\r\n",
    "        before_dt = dt.datetime.fromtimestamp(before_dt)\r\n",
    "        after_dt = dt.datetime.fromtimestamp(after_dt)\r\n",
    "        end_dt = dt.datetime.fromtimestamp(end)\r\n",
    "\r\n",
    "        # Create dataframe\r\n",
    "        comments_df = pd.DataFrame(comments)\r\n",
    "\r\n",
    "        # Create list of columns to keep\r\n",
    "        cols = ['author', 'author_fullname', 'author_premium', 'body', 'collapsed_reason_code', 'comment_type', 'created_utc', 'score',\r\n",
    "                'id', 'parent_id', 'permalink']\r\n",
    "\r\n",
    "        # Make new dataframe with above columns\r\n",
    "        df = comments_df[cols].copy()\r\n",
    "\r\n",
    "        # Replace all characters except for letters and numbers\r\n",
    "        #comments_df['body'] = comments_df['body'].str.replace(\"(?i)[^0-9a-z!?.;,@' -]\",' ')\r\n",
    "\r\n",
    "        # Clean datetime for csv filename\r\n",
    "        date = str(after_dt).split(' 00:00:00', 1)[0]\r\n",
    "\r\n",
    "        # Export dataframe to csv\r\n",
    "        df.to_csv('../Data/Reddit_Comments/Blockchain/' + subreddit + '_' + date + '.csv', header=True, index=False, columns=list(df.axes[1]))\r\n",
    "\r\n",
    "        # Export to pickle file\r\n",
    "        #df.to_pickle('../Data/Reddit_Comments/Cryptocurrency_pkl/' + subreddit + '_' + date + '.csv')\r\n",
    "\r\n",
    "        # sleep for n seconds\r\n",
    "        sleep(5)\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "99993 result(s) not found in Pushshift\n",
      "Total:: Success Rate: 100.00% - Requests: 10 - Batches: 1 - Items Remaining: 0\n",
      "Retrieved 7 comments from Pushshift\n",
      "99978 result(s) not found in Pushshift\n",
      "Total:: Success Rate: 100.00% - Requests: 8 - Batches: 1 - Items Remaining: 0\n",
      "Retrieved 22 comments from Pushshift\n",
      "99987 result(s) not found in Pushshift\n",
      "Total:: Success Rate: 100.00% - Requests: 10 - Batches: 1 - Items Remaining: 0\n",
      "Retrieved 13 comments from Pushshift\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write to Postgres database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install psycopg2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#from sqlalchemy import create_engine\r\n",
    "#import psycopg2 \r\n",
    "#import io"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#engine = create_engine('postgresql+psycopg2://username:password@host:port/database')\r\n",
    "\r\n",
    "#df.head(0).to_sql('table_name', engine, if_exists='replace',index=False) #drops old table and creates new empty table\r\n",
    "\r\n",
    "#conn = engine.raw_connection()\r\n",
    "#cur = conn.cursor()\r\n",
    "#output = io.StringIO()\r\n",
    "#df.to_csv(output, sep='\\t', header=False, index=False)\r\n",
    "#output.seek(0)\r\n",
    "#contents = output.getvalue()\r\n",
    "#cur.copy_from(output, 'table_name', null=\"\") # null values become ''\r\n",
    "#conn.commit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write pandas dataframe to parquet file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install pyarrow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write Pandas Dataframe to parquet\r\n",
    "#import pyarrow as pa\r\n",
    "#import pyarrow.parquet as pq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert DataFrame to Apache Arrow Table\r\n",
    "#table = pa.Table.from_pandas(comments_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parquet write table\r\n",
    "#pq.write_table(table, 'file_name.parquet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parquet with GZIP compression\r\n",
    "#pq.write_table(table, '../Data/Reddit_Comments/Cryptocurrency_09012021.parquet', compression='GZIP')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create dataframe\r\n",
    "#p = pd.DataFrame(posts_from_reddit)\r\n",
    "#c = pd.DataFrame(comments_from_reddit)\r\n",
    "    \r\n",
    "# Create date string for csv file name\r\n",
    "#timestr = time.strftime(\"%Y%m%d\")\r\n",
    "    \r\n",
    "# Save dataframe to csv file\r\n",
    "#p.to_csv(csv_dir + subreddit + 'posts_' + timestr + '.csv', index = False)\r\n",
    "#c.to_csv(csv_dir + subreddit + 'comments_' + timestr + '.csv', index = False)\r\n",
    "\r\n",
    "# Save dataframe to parquet file\r\n",
    "#p.to_parquet(parquet_dir + subreddit + '_' + timestr + '.parquet', engine='fastparquet')\r\n",
    "#c.to_parquet(parquet_dir + subreddit + '_' + timestr + '.parquet', engine='fastparquet')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "825893eda577408078809c25d9ed95f592e592429a99ce56af90a73b72386c66"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
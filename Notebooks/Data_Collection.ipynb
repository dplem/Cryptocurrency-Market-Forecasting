{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection from Reddit Cryptocurrency Subreddits\r\n",
    "\r\n",
    "https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\r\n",
    "\r\n",
    "https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install pmaw"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\r\n",
    "import json\r\n",
    "import requests\r\n",
    "import itertools\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "from datetime import datetime, timedelta\r\n",
    "import praw\r\n",
    "import pandas as pd\r\n",
    "from pmaw import PushshiftAPI\r\n",
    "import datetime as dt\r\n",
    "from time import sleep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the name of the directory to be created. Replace with your directory location.\r\n",
    "csv_dir = '../Data/Reddit_Comments/'\r\n",
    "\r\n",
    "# Define directory for parquet file.\r\n",
    "parquet_dir = '../Data/Parquet/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sub_list = ['Bitcoin']\r\n",
    "\r\n",
    "# 'Cryptocurrency, 'Altcoin', 'Bitcoin', 'Ethereum', 'BasicAttentionToken', 'Best_of_Crypto', 'BitcoinMarkets', \r\n",
    "#            'Blockchain', 'CryptoMarkets', 'CryptoTechnology', 'CryptoTrade', 'Algorand', 'Tezos', 'cosmosnetwork',\r\n",
    "#            'Polkadot', 'Cardano', 'Ankr']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "day = dt.timedelta(days=1)\r\n",
    "\r\n",
    "for x in sub_list:\r\n",
    "\r\n",
    "    #set date intervals\r\n",
    "    before = dt.datetime(2021,9,10,0,0).timestamp()\r\n",
    "    after = dt.datetime(2021,9,9,0,0).timestamp()\r\n",
    "    end = dt.datetime(2021,8,9,0,0).timestamp()\r\n",
    "\r\n",
    "    # Convert timestamp to datetime.datetime\r\n",
    "    before_dt = dt.datetime.fromtimestamp(before)\r\n",
    "    after_dt = dt.datetime.fromtimestamp(after)\r\n",
    "    end_dt = dt.datetime.fromtimestamp(end)\r\n",
    "\r\n",
    "    # Lowers date by 1 day for every iteration of the loop\r\n",
    "    while before_dt >= end_dt:\r\n",
    "        before_dt = int((before_dt - day).timestamp())\r\n",
    "        after_dt = int((after_dt - day).timestamp())\r\n",
    "        \r\n",
    "        subreddit = x\r\n",
    "\r\n",
    "        # pmaw python library to pull comments\r\n",
    "        api = PushshiftAPI()\r\n",
    "        subreddit = x\r\n",
    "        limit=100000\r\n",
    "        comments = api.search_comments(subreddit=subreddit, limit=limit, before=before_dt, after=after_dt)\r\n",
    "        print(f'Retrieved {len(comments)} comments from Pushshift')\r\n",
    "\r\n",
    "        # Convert timestamp to datetime.datetime\r\n",
    "        before_dt = dt.datetime.fromtimestamp(before_dt)\r\n",
    "        after_dt = dt.datetime.fromtimestamp(after_dt)\r\n",
    "        end_dt = dt.datetime.fromtimestamp(end)\r\n",
    "\r\n",
    "        # Create dataframe\r\n",
    "        comments_df = pd.DataFrame(comments)\r\n",
    "\r\n",
    "        # Create list of columns to keep\r\n",
    "        cols = ['author', 'author_fullname', 'author_premium', 'body', 'collapsed_reason_code', 'comment_type', 'created_utc', 'score',\r\n",
    "                'id', 'parent_id', 'permalink']\r\n",
    "\r\n",
    "        # Make new dataframe with above columns\r\n",
    "        df = comments_df[cols].copy()\r\n",
    "\r\n",
    "        # Replace all characters except for letters and numbers\r\n",
    "        #comments_df['body'] = comments_df['body'].str.replace(\"(?i)[^0-9a-z!?.;,@' -]\",' ')\r\n",
    "\r\n",
    "        # Clean datetime for csv filename\r\n",
    "        date = str(after_dt).split(' 00:00:00', 1)[0]\r\n",
    "\r\n",
    "        # Export dataframe to csv\r\n",
    "        df.to_csv('../Data/Reddit_Comments/Blockchain/' + subreddit + '_' + date + '.csv', header=True, index=False, columns=list(df.axes[1]))\r\n",
    "\r\n",
    "        # Export to pickle file\r\n",
    "        #df.to_pickle('../Data/Reddit_Comments/Cryptocurrency_pkl/' + subreddit + '_' + date + '.csv')\r\n",
    "\r\n",
    "        # sleep for n seconds\r\n",
    "        sleep(5)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cryptocurrency Historical Price Data Collection\r\n",
    "\r\n",
    "https://pypi.org/project/Historic-Crypto/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install Historic-Crypto"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting Historic-Crypto\n",
      "  Downloading Historic_Crypto-0.1.4-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user1\\anaconda3\\lib\\site-packages (from Historic-Crypto) (4.59.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user1\\anaconda3\\lib\\site-packages (from Historic-Crypto) (1.2.4)\n",
      "Requirement already satisfied: requests in c:\\users\\user1\\anaconda3\\lib\\site-packages (from Historic-Crypto) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from pandas->Historic-Crypto) (1.20.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from pandas->Historic-Crypto) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from pandas->Historic-Crypto) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->Historic-Crypto) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from requests->Historic-Crypto) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from requests->Historic-Crypto) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from requests->Historic-Crypto) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user1\\anaconda3\\lib\\site-packages (from requests->Historic-Crypto) (1.26.4)\n",
      "Installing collected packages: Historic-Crypto\n",
      "Successfully installed Historic-Crypto-0.1.4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "from Historic_Crypto import HistoricalData, Cryptocurrencies"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Get full list of cryptocurrencies\r\n",
    "data = Cryptocurrencies(extended_output=False).find_crypto_pairs()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Connected to the CoinBase Pro API.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Export historical data\r\n",
    "new = HistoricalData('BTC-USD',21600,'2010-01-01-00-00').retrieve_data()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking input parameters are in the correct format...\n",
      "Formatting Dates...\n",
      "Checking if ticker supplied is available on the CoinBase Pro API...\n",
      "Connected to the CoinBase Pro API...\n",
      "Ticker 'BTC-USD' found at the CoinBase Pro API, continuing to extraction...\n",
      "Data for chunk 0 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2010-01-01T00:00:00'\n",
      "Data for chunk 1 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2010-03-17T00:00:00'\n",
      "Data for chunk 2 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2010-05-31T00:00:00'\n",
      "Data for chunk 3 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2010-08-14T00:00:00'\n",
      "Data for chunk 4 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2010-10-28T00:00:00'\n",
      "Data for chunk 5 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2011-01-11T00:00:00'\n",
      "Data for chunk 6 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2011-03-27T00:00:00'\n",
      "Data for chunk 7 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2011-06-10T00:00:00'\n",
      "Data for chunk 8 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2011-08-24T00:00:00'\n",
      "Data for chunk 9 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2011-11-07T00:00:00'\n",
      "Data for chunk 10 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2012-01-21T00:00:00'\n",
      "Data for chunk 11 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2012-04-05T00:00:00'\n",
      "Data for chunk 12 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2012-06-19T00:00:00'\n",
      "Data for chunk 13 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2012-09-02T00:00:00'\n",
      "Data for chunk 14 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2012-11-16T00:00:00'\n",
      "Data for chunk 15 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2013-01-30T00:00:00'\n",
      "Data for chunk 16 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2013-04-15T00:00:00'\n",
      "Data for chunk 17 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2013-06-29T00:00:00'\n",
      "Data for chunk 18 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2013-09-12T00:00:00'\n",
      "Data for chunk 19 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2013-11-26T00:00:00'\n",
      "Data for chunk 20 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2014-02-09T00:00:00'\n",
      "Data for chunk 21 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2014-04-25T00:00:00'\n",
      "Data for chunk 22 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2014-07-09T00:00:00'\n",
      "Data for chunk 23 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2014-09-22T00:00:00'\n",
      "Data for chunk 24 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2014-12-06T00:00:00'\n",
      "Data for chunk 25 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2015-02-19T00:00:00'\n",
      "Data for chunk 26 of 56 extracted\n",
      "CoinBase Pro API did not have any data available for 'BTC-USD' beginning at 2010-01-01-00-00. Trying a later date:'2015-05-05T00:00:00'\n",
      "Data for chunk 27 of 56 extracted\n",
      "Data for chunk 28 of 56 extracted\n",
      "Data for chunk 29 of 56 extracted\n",
      "Data for chunk 30 of 56 extracted\n",
      "Data for chunk 31 of 56 extracted\n",
      "Data for chunk 32 of 56 extracted\n",
      "Data for chunk 33 of 56 extracted\n",
      "Data for chunk 34 of 56 extracted\n",
      "Data for chunk 35 of 56 extracted\n",
      "Data for chunk 36 of 56 extracted\n",
      "Data for chunk 37 of 56 extracted\n",
      "Data for chunk 38 of 56 extracted\n",
      "Data for chunk 39 of 56 extracted\n",
      "Data for chunk 40 of 56 extracted\n",
      "Data for chunk 41 of 56 extracted\n",
      "Data for chunk 42 of 56 extracted\n",
      "Data for chunk 43 of 56 extracted\n",
      "Data for chunk 44 of 56 extracted\n",
      "Data for chunk 45 of 56 extracted\n",
      "Data for chunk 46 of 56 extracted\n",
      "Data for chunk 47 of 56 extracted\n",
      "Data for chunk 48 of 56 extracted\n",
      "Data for chunk 49 of 56 extracted\n",
      "Data for chunk 50 of 56 extracted\n",
      "Data for chunk 51 of 56 extracted\n",
      "Data for chunk 52 of 56 extracted\n",
      "Data for chunk 53 of 56 extracted\n",
      "Data for chunk 54 of 56 extracted\n",
      "Data for chunk 55 of 56 extracted\n",
      "Data for chunk 56 of 56 extracted\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "new"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-07-20 18:00:00</th>\n",
       "      <td>277.37</td>\n",
       "      <td>280.00</td>\n",
       "      <td>277.98</td>\n",
       "      <td>280.00</td>\n",
       "      <td>782.883420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-21 00:00:00</th>\n",
       "      <td>279.38</td>\n",
       "      <td>281.27</td>\n",
       "      <td>279.96</td>\n",
       "      <td>280.81</td>\n",
       "      <td>1480.194721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-21 06:00:00</th>\n",
       "      <td>278.76</td>\n",
       "      <td>280.89</td>\n",
       "      <td>280.81</td>\n",
       "      <td>279.40</td>\n",
       "      <td>602.330470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-21 12:00:00</th>\n",
       "      <td>278.25</td>\n",
       "      <td>280.00</td>\n",
       "      <td>279.38</td>\n",
       "      <td>279.76</td>\n",
       "      <td>1177.272342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-21 18:00:00</th>\n",
       "      <td>276.85</td>\n",
       "      <td>280.00</td>\n",
       "      <td>279.76</td>\n",
       "      <td>277.32</td>\n",
       "      <td>1683.761901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-10 00:00:00</th>\n",
       "      <td>46268.41</td>\n",
       "      <td>47040.76</td>\n",
       "      <td>46396.26</td>\n",
       "      <td>46697.74</td>\n",
       "      <td>2598.950551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-10 06:00:00</th>\n",
       "      <td>45650.00</td>\n",
       "      <td>46717.53</td>\n",
       "      <td>46698.09</td>\n",
       "      <td>46339.42</td>\n",
       "      <td>2947.972725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-10 12:00:00</th>\n",
       "      <td>44700.00</td>\n",
       "      <td>46490.80</td>\n",
       "      <td>46339.41</td>\n",
       "      <td>45718.54</td>\n",
       "      <td>5454.118717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-10 18:00:00</th>\n",
       "      <td>44140.48</td>\n",
       "      <td>45829.63</td>\n",
       "      <td>45716.87</td>\n",
       "      <td>44851.45</td>\n",
       "      <td>3904.505961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-11 00:00:00</th>\n",
       "      <td>44730.29</td>\n",
       "      <td>45483.00</td>\n",
       "      <td>44850.37</td>\n",
       "      <td>45395.67</td>\n",
       "      <td>709.170575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8977 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          low      high      open     close       volume\n",
       "time                                                                    \n",
       "2015-07-20 18:00:00    277.37    280.00    277.98    280.00   782.883420\n",
       "2015-07-21 00:00:00    279.38    281.27    279.96    280.81  1480.194721\n",
       "2015-07-21 06:00:00    278.76    280.89    280.81    279.40   602.330470\n",
       "2015-07-21 12:00:00    278.25    280.00    279.38    279.76  1177.272342\n",
       "2015-07-21 18:00:00    276.85    280.00    279.76    277.32  1683.761901\n",
       "...                       ...       ...       ...       ...          ...\n",
       "2021-09-10 00:00:00  46268.41  47040.76  46396.26  46697.74  2598.950551\n",
       "2021-09-10 06:00:00  45650.00  46717.53  46698.09  46339.42  2947.972725\n",
       "2021-09-10 12:00:00  44700.00  46490.80  46339.41  45718.54  5454.118717\n",
       "2021-09-10 18:00:00  44140.48  45829.63  45716.87  44851.45  3904.505961\n",
       "2021-09-11 00:00:00  44730.29  45483.00  44850.37  45395.67   709.170575\n",
       "\n",
       "[8977 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "new.to_csv('../Data/Historical/' + 'BTC' + '.csv', header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write to Postgres database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install psycopg2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#from sqlalchemy import create_engine\r\n",
    "#import psycopg2 \r\n",
    "#import io"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#engine = create_engine('postgresql+psycopg2://username:password@host:port/database')\r\n",
    "\r\n",
    "#df.head(0).to_sql('table_name', engine, if_exists='replace',index=False) #drops old table and creates new empty table\r\n",
    "\r\n",
    "#conn = engine.raw_connection()\r\n",
    "#cur = conn.cursor()\r\n",
    "#output = io.StringIO()\r\n",
    "#df.to_csv(output, sep='\\t', header=False, index=False)\r\n",
    "#output.seek(0)\r\n",
    "#contents = output.getvalue()\r\n",
    "#cur.copy_from(output, 'table_name', null=\"\") # null values become ''\r\n",
    "#conn.commit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write pandas dataframe to parquet file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install pyarrow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write Pandas Dataframe to parquet\r\n",
    "#import pyarrow as pa\r\n",
    "#import pyarrow.parquet as pq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert DataFrame to Apache Arrow Table\r\n",
    "#table = pa.Table.from_pandas(comments_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parquet write table\r\n",
    "#pq.write_table(table, 'file_name.parquet')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parquet with GZIP compression\r\n",
    "#pq.write_table(table, '../Data/Reddit_Comments/Cryptocurrency_09012021.parquet', compression='GZIP')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create dataframe\r\n",
    "#p = pd.DataFrame(posts_from_reddit)\r\n",
    "#c = pd.DataFrame(comments_from_reddit)\r\n",
    "    \r\n",
    "# Create date string for csv file name\r\n",
    "#timestr = time.strftime(\"%Y%m%d\")\r\n",
    "    \r\n",
    "# Save dataframe to csv file\r\n",
    "#p.to_csv(csv_dir + subreddit + 'posts_' + timestr + '.csv', index = False)\r\n",
    "#c.to_csv(csv_dir + subreddit + 'comments_' + timestr + '.csv', index = False)\r\n",
    "\r\n",
    "# Save dataframe to parquet file\r\n",
    "#p.to_parquet(parquet_dir + subreddit + '_' + timestr + '.parquet', engine='fastparquet')\r\n",
    "#c.to_parquet(parquet_dir + subreddit + '_' + timestr + '.parquet', engine='fastparquet')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "825893eda577408078809c25d9ed95f592e592429a99ce56af90a73b72386c66"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}